{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02eebbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09ed946e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distancia_euclidiana_km(lat1, lon1, lat2, lon2):\n",
    "    # entrada: floats (graus). retorno: distância em km (aprox.)\n",
    "    return np.sqrt((lat1 - lat2)**2 + (lon1 - lon2)**2) * 111.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef26c2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cimehgo = r\"C:\\Users\\thiag\\OneDrive\\Área de Trabalho\\TCC 2\\Scripts\\Arquivos Finais\\CIMEHGO_FINAL.xlsx\"\n",
    "path_inmet   = r\"C:\\Users\\thiag\\OneDrive\\Área de Trabalho\\TCC 2\\Scripts\\Arquivos Finais\\INMET_FINAL.xlsx\"\n",
    "\n",
    "path_locais  = r\"C:\\Users\\thiag\\OneDrive\\Área de Trabalho\\TCC 2\\Scripts\\Arquivos Finais\\Locais_por_Data_GEOCODED_FINAL.xlsx\"\n",
    "path_relatos = r\"C:\\Users\\thiag\\OneDrive\\Área de Trabalho\\TCC 2\\Scripts\\Arquivos Finais\\RELATOS_GEOCODED_FINAL.xlsx\"\n",
    "path_bombe   = r\"C:\\Users\\thiag\\OneDrive\\Área de Trabalho\\TCC 2\\Scripts\\Arquivos Finais\\BOMBEIROS_GEOCODED_FINAL.xlsx\"\n",
    "\n",
    "out_folder_bi = r\"C:\\Users\\thiag\\OneDrive\\Área de Trabalho\\TCC 2\\Scripts\\Arquivos para BI\"\n",
    "os.makedirs(out_folder_bi, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe672a3",
   "metadata": {},
   "source": [
    "# Estações"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3921ac58",
   "metadata": {},
   "outputs": [],
   "source": [
    "cimehgo = pd.read_excel(path_cimehgo)\n",
    "inmet   = pd.read_excel(path_inmet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e58b1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cimehgo_p = cimehgo.rename(columns={\n",
    "    \"ESTAÇÃO\": \"Estacao\",\n",
    "    \"DATA/HORA(LOCAL)\": \"Data\",\n",
    "    \"CHUVA(MM)\": \"Chuva_mm\"\n",
    "})\n",
    "inmet_p = inmet.rename(columns={\n",
    "    \"ESTACAO\": \"Estacao\",\n",
    "    \"Data\": \"Data\",\n",
    "    \"Chuva(mm)\": \"Chuva_mm\",                   \n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a590fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inmet_p[\"Chuva_mm\"]   = pd.to_numeric(inmet_p[\"Chuva_mm\"], errors=\"coerce\").fillna(0)\n",
    "cimehgo_p[\"Chuva_mm\"] = pd.to_numeric(cimehgo_p[\"Chuva_mm\"], errors=\"coerce\").fillna(0)\n",
    "\n",
    "\n",
    "if \"Hora\" in inmet_p.columns:\n",
    "    inmet_p[\"Hora\"] = pd.to_datetime(inmet_p[\"Hora\"], format=\"%H:%M\", errors=\"coerce\").dt.time\n",
    "    inmet_p[\"Data\"] = pd.to_datetime(inmet_p[\"Data\"], errors=\"coerce\")\n",
    "    inmet_p[\"Data\"] = inmet_p.apply(lambda row: pd.Timestamp.combine(row[\"Data\"], row[\"Hora\"]) if pd.notna(row[\"Data\"]) and pd.notna(row[\"Hora\"]) else row[\"Data\"], axis=1)\n",
    "else:\n",
    "    inmet_p[\"Data\"] = pd.to_datetime(inmet_p[\"Data\"], errors=\"coerce\")\n",
    "\n",
    "cimehgo_p[\"Data\"] = pd.to_datetime(cimehgo_p[\"Data\"], errors=\"coerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bfd2fdd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cimehgo_sel = cimehgo_p[[\"Estacao\", \"Data\", \"Chuva_mm\", \"LATITUDE\", \"LONGITUDE\"]]\n",
    "inmet_sel   = inmet_p[[\"Estacao\", \"Data\", \"Chuva_mm\", \"LATITUDE\", \"LONGITUDE\"]]\n",
    "\n",
    "df_chuva_unificada = pd.concat([cimehgo_sel, inmet_sel], ignore_index=True)\n",
    "df_chuva_unificada[\"data_dia\"] = df_chuva_unificada[\"Data\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06c9a267",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chuva_unificada.to_csv(os.path.join(out_folder_bi, \"DADOS_UNIFICADOS_ESTACOES.csv\"), index=False, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00665843",
   "metadata": {},
   "source": [
    "# Ocorrências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02cb6418",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp = pd.read_excel(path_locais)\n",
    "rp = pd.read_excel(path_relatos)\n",
    "bp = pd.read_excel(path_bombe)\n",
    "\n",
    "bp = bp.rename(columns={\"ENDEREÇO\": \"LOCAL\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e784762",
   "metadata": {},
   "outputs": [],
   "source": [
    "locais   = lp[[\"DATA\", \"LOCAL\", \"Estação atribuída\", \"LATITUDE\", \"LONGITUDE\"]].copy()\n",
    "relatos  = rp[[\"DATA\", \"LOCAL\", \"Estação atribuída\", \"LATITUDE\", \"LONGITUDE\"]].copy()\n",
    "bombeiros= bp[[\"DATA\", \"LOCAL\", \"Estação atribuída\", \"LATITUDE\", \"LONGITUDE\"]].copy()\n",
    "\n",
    "locais[\"Fonte\"] = \"Noticias\"\n",
    "relatos[\"Fonte\"] = \"Relatos\"\n",
    "bombeiros[\"Fonte\"] = \"Bombeiros\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5595bda9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thiag\\AppData\\Local\\Temp\\ipykernel_22412\\1656189541.py:10: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"raise\", infer_datetime_format=True)\n",
      "C:\\Users\\thiag\\AppData\\Local\\Temp\\ipykernel_22412\\1656189541.py:10: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"raise\", infer_datetime_format=True)\n",
      "C:\\Users\\thiag\\AppData\\Local\\Temp\\ipykernel_22412\\1656189541.py:10: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  return pd.to_datetime(s, errors=\"raise\", infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "def parse_date(x):\n",
    "    if pd.isna(x):\n",
    "        return pd.NaT\n",
    "    s = str(x).strip()\n",
    "    if \",\" in s:\n",
    "        partes = s.split(\",\", 1)\n",
    "        if len(partes) == 2:\n",
    "            s = partes[1].strip()\n",
    "    try:\n",
    "        return pd.to_datetime(s, errors=\"raise\", infer_datetime_format=True)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        return datetime.strptime(s, \"%B %d %Y\")\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        return datetime.strptime(s, \"%B %d, %Y\")\n",
    "    except:\n",
    "        pass\n",
    "    return pd.NaT\n",
    "\n",
    "locais[\"DATA\"] = locais[\"DATA\"].apply(parse_date)\n",
    "relatos[\"DATA\"] = relatos[\"DATA\"].apply(parse_date)\n",
    "bombeiros[\"DATA\"] = bombeiros[\"DATA\"].apply(parse_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b964c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocorrencias = pd.concat([locais, relatos, bombeiros], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bfdf409",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ocorrencias[\"DATA\"] = pd.to_datetime(df_ocorrencias[\"DATA\"], errors=\"coerce\")\n",
    "df_ocorrencias[\"data_dia\"] = df_ocorrencias[\"DATA\"].dt.date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0c13ec",
   "metadata": {},
   "source": [
    "# Analise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fc01a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_meta = df_chuva_unificada[[\"Estacao\", \"LATITUDE\", \"LONGITUDE\"]].drop_duplicates().dropna(subset=[\"LATITUDE\", \"LONGITUDE\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a5e195a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_lat = stations_meta[\"LATITUDE\"].to_numpy(dtype=float)\n",
    "stations_lon = stations_meta[\"LONGITUDE\"].to_numpy(dtype=float)\n",
    "stations_names = stations_meta[\"Estacao\"].to_numpy(dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e56a016d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Processando raio 1.5 km ===\n",
      "Raio 1.5 km -> ocorrencias atribuídas: 107 / 363 | estações usadas: 13\n",
      "Salvos: DADOS_GEOCODED_UNIFICADOS_1,5km.xlsx , ESTATISTICAS_EVENTOS_1,5km.xlsx , RESUMO CSV\n",
      "\n",
      "=== Processando raio 5 km ===\n",
      "Raio 5 km -> ocorrencias atribuídas: 360 / 363 | estações usadas: 16\n",
      "Salvos: DADOS_GEOCODED_UNIFICADOS_5km.xlsx , ESTATISTICAS_EVENTOS_5km.xlsx , RESUMO CSV\n",
      "\n",
      "=== Processando raio 10 km ===\n",
      "Raio 10 km -> ocorrencias atribuídas: 363 / 363 | estações usadas: 16\n",
      "Salvos: DADOS_GEOCODED_UNIFICADOS_10km.xlsx , ESTATISTICAS_EVENTOS_10km.xlsx , RESUMO CSV\n",
      "\n",
      "=== Processando raio 20 km ===\n",
      "Raio 20 km -> ocorrencias atribuídas: 363 / 363 | estações usadas: 16\n",
      "Salvos: DADOS_GEOCODED_UNIFICADOS_20km.xlsx , ESTATISTICAS_EVENTOS_20km.xlsx , RESUMO CSV\n",
      "\n",
      "=== FIM DO PROCESSAMENTO PARA TODOS OS RAIOS ===\n"
     ]
    }
   ],
   "source": [
    "RAIOS = [1.5, 5, 10, 20]\n",
    "\n",
    "# -----------------------\n",
    "# Loop principal: para cada raio, reatribui ocorrencias -> junta chuva -> salva outputs\n",
    "# -----------------------\n",
    "for raio_km in RAIOS:\n",
    "    print(f\"\\n=== Processando raio {raio_km} km ===\")\n",
    "    # copia do df de ocorrências para manipular\n",
    "    occ = df_ocorrencias.copy().reset_index(drop=True)\n",
    "\n",
    "    # prepara colunas de atribuição\n",
    "    occ[\"Estacao_reatribuida\"] = None\n",
    "    occ[\"Dist_km_reatribuida\"] = None\n",
    "\n",
    "    # itera sobre ocorrências (vectorização parcial)\n",
    "    # extrai arrays de lat/lon ocorrência\n",
    "    occ_lat = occ[\"LATITUDE\"].to_numpy(dtype=float)\n",
    "    occ_lon = occ[\"LONGITUDE\"].to_numpy(dtype=float)\n",
    "\n",
    "    # Para cada ocorrência calcula distâncias para todas as estações e escolhe a mais próxima dentro do raio\n",
    "    for i in range(len(occ)):\n",
    "        lat_o = occ_lat[i]\n",
    "        lon_o = occ_lon[i]\n",
    "        if np.isnan(lat_o) or np.isnan(lon_o):\n",
    "            continue\n",
    "        # calcula vetor de distâncias (km)\n",
    "        dists = distancia_euclidiana_km(lat_o, lon_o, stations_lat, stations_lon)\n",
    "        # encontra índices dentro do raio\n",
    "        within_idx = np.where(dists <= raio_km)[0]\n",
    "        if within_idx.size == 0:\n",
    "            # nenhuma estação dentro do raio\n",
    "            occ.at[i, \"Estacao_reatribuida\"] = \"sem estação correspondente\"\n",
    "            occ.at[i, \"Dist_km_reatribuida\"] = np.nan\n",
    "        else:\n",
    "            # escolhe a estação com menor distância\n",
    "            best = within_idx[np.argmin(dists[within_idx])]\n",
    "            occ.at[i, \"Estacao_reatribuida\"] = stations_names[best]\n",
    "            occ.at[i, \"Dist_km_reatribuida\"] = float(np.round(dists[best], 4))\n",
    "\n",
    "    # normalizar nome da coluna para o merge com df_chuva_unificada\n",
    "    occ_for_merge = occ.rename(columns={\"Estacao_reatribuida\": \"Estacao\", \"DATA\": \"DATA_ocorrencia\"})\n",
    "\n",
    "    # filtra apenas ocorrências cuja estação atribuída existe na base de chuva\n",
    "    valid_stations = set(df_chuva_unificada[\"Estacao\"].unique())\n",
    "    occ_for_merge = occ_for_merge[ occ_for_merge[\"Estacao\"].isin(valid_stations) ].copy()\n",
    "\n",
    "    # cria data_dia (já existe) -> garantir nome igual\n",
    "    occ_for_merge[\"data_dia\"] = pd.to_datetime(occ_for_merge[\"data_dia\"])\n",
    "\n",
    "    # PREPARA df_chuva_unificada para merge: garantir data_dia como date (sem horario)\n",
    "    chuva = df_chuva_unificada.copy()\n",
    "    chuva[\"data_dia\"] = pd.to_datetime(chuva[\"data_dia\"])\n",
    "\n",
    "    # Executa merge por Estacao + data_dia (left join: mantém ocorrências)\n",
    "    df_expand = pd.merge(\n",
    "        occ_for_merge,\n",
    "        chuva,\n",
    "        on=[\"Estacao\", \"data_dia\"],\n",
    "        how=\"left\",\n",
    "        suffixes=(\"_ocorr\", \"_chuva\")\n",
    "    )\n",
    "\n",
    "    # Estatísticas por LOCAL/DATA/Estacao (min/max/mean de Chuva_mm)\n",
    "    estat = (\n",
    "        df_expand\n",
    "        .groupby(['LOCAL', 'DATA_ocorrencia', 'Estacao'])['Chuva_mm']\n",
    "        .agg(['min','max','mean'])\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    # Salva arquivos de saída específicos para este raio\n",
    "    nome_base = f\"DADOS_GEOCODED_UNIFICADOS_{str(raio_km).replace('.',',')}km.xlsx\"\n",
    "    caminho_out_expand = os.path.join(out_folder_bi, nome_base)\n",
    "    df_expand.to_excel(caminho_out_expand, index=False)\n",
    "\n",
    "    nome_estat = f\"ESTATISTICAS_EVENTOS_{str(raio_km).replace('.',',')}km.xlsx\"\n",
    "    caminho_out_estat = os.path.join(out_folder_bi, nome_estat)\n",
    "    estat.to_excel(caminho_out_estat, index=False)\n",
    "\n",
    "    # resumo: contagens\n",
    "    total_ocorr = len(occ)\n",
    "    atribuídas = df_expand['LOCAL'].nunique() if 'LOCAL' in df_expand.columns else len(df_expand)\n",
    "    sem_estacao = total_ocorr - len(occ_for_merge)\n",
    "    estações_usadas = df_expand['Estacao'].nunique()\n",
    "    resumo = {\n",
    "        \"raio_km\": raio_km,\n",
    "        \"total_ocorrencias_input\": total_ocorr,\n",
    "        \"ocorrencias_com_estacao_dentro_raio\": len(occ_for_merge),\n",
    "        \"ocorrencias_sem_estacao_dentro_raio\": sem_estacao,\n",
    "        \"estações_distintas_usadas\": estações_usadas\n",
    "    }\n",
    "    resumo_df = pd.DataFrame([resumo])\n",
    "    resumo_df.to_csv(os.path.join(out_folder_bi, f\"RESUMO_ATRIBUICAO_{str(raio_km).replace('.',',')}km.csv\"), index=False)\n",
    "\n",
    "    print(f\"Raio {raio_km} km -> ocorrencias atribuídas: {len(occ_for_merge)} / {total_ocorr} | estações usadas: {estações_usadas}\")\n",
    "    print(f\"Salvos: {os.path.basename(caminho_out_expand)} , {os.path.basename(caminho_out_estat)} , RESUMO CSV\")\n",
    "\n",
    "print(\"\\n=== FIM DO PROCESSAMENTO PARA TODOS OS RAIOS ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76085b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
